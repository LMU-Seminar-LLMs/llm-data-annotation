{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-16T10:45:06.607069300Z",
     "start_time": "2023-08-16T10:45:05.070391200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tenacity import (\n",
    "retry,\n",
    "stop_after_attempt,\n",
    "wait_random_exponential,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\saran\\\\Desktop\\\\LLM Seminar\\\\Apps Phase\\\\LLM_Data_Annotation\\\\notebooks'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T10:45:06.628077300Z",
     "start_time": "2023-08-16T10:45:06.607069300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T10:45:06.641848800Z",
     "start_time": "2023-08-16T10:45:06.624534Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/merged/merged_2.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T10:52:27.804129600Z",
     "start_time": "2023-08-16T10:52:27.770388200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 text  confidence_scores  \\\n0                 I`d have responded, if I were going           1.000000   \n1       Sooo SAD I will miss you here in San Diego!!!           1.000000   \n2                           my boss is bullying me...           1.000000   \n3                      what interview! leave me alone           1.000000   \n4    Sons of ****, why couldn`t they put them on t...           1.000000   \n5   http://www.dothebouncy.com/smf - some shameles...           1.000000   \n6   2am feedings for the baby are fun when he is a...           1.000000   \n7                                          Soooo high           1.000000   \n8                                         Both of you           1.000000   \n9    Journey!? Wow... u just became cooler.  hehe....           1.000000   \n10   as much as i love to be hopeful, i reckon the...           1.000000   \n11  I really really like the song Love Story by Ta...           1.000000   \n12       My Sharpie is running DANGERously low on ink           1.000000   \n13  i want to go to music tonight but i lost my vo...           1.000000   \n14                         test test from the LG enV2           1.000000   \n15                              Uh oh, I am sunburned           1.000000   \n16   S`ok, trying to plot alternatives as we speak...           0.666667   \n17  i`ve been sick for the past few days  and thus...           1.000000   \n18         is back home now      gonna miss every one           1.000000   \n19                         Hes just not that into you           1.000000   \n20   oh Marly, I`m so sorry!!  I hope you find her...           1.000000   \n21  Playing Ghost Online is really interesting. Th...           1.000000   \n22  is cleaning the house for her family who is co...           0.666667   \n23  gotta restart my computer .. I thought Win7 wa...           1.000000   \n24  SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...           1.000000   \n25  the free fillin` app on my ipod is fun, im add...           1.000000   \n26                                         I`m sorry.           0.333333   \n27  On the way to Malaysia...no internet access to...           1.000000   \n28  juss came backk from Berkeleyy ; omg its madd ...           1.000000   \n29  Went to sleep and there is a power cut in Noid...           1.000000   \n30  I`m going home now. Have you seen my new twitt...           1.000000   \n31  i hope unni will make the audition . fighting ...           1.000000   \n32   If it is any consolation I got my BMI tested ...           1.000000   \n33                     That`s very funny.  Cute kids.           1.000000   \n34   Ahhh, I slept through the game.  I`m gonna tr...           0.666667   \n35  Thats it, its the end. Tears for Fears vs Eric...           1.000000   \n36  Born and raised in NYC and living in Texas for...           1.000000   \n37  just in case you wonder, we are really busy to...           1.000000   \n38  i`m soooooo sleeeeepy!!! the last day o` schoo...           1.000000   \n39  A little happy for the wine jeje ok it`sm my f...           1.000000   \n40   Car not happy, big big dent in boot! Hoping t...           1.000000   \n41  im an avid fan of **** magazine and i love you...           1.000000   \n42                                           MAYDAY?!           1.000000   \n43  RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED...           1.000000   \n44   I love to! But I`m only available from 5pm.  ...           1.000000   \n45  The girl in the hair salon asked me 'Shall I t...           1.000000   \n46  egh blah and boooooooooooo i dunno wanna go to...           1.000000   \n47               :visiting my friendster and facebook           1.000000   \n48  i donbt like to peel prawns, i also dont like ...           1.000000   \n49   which case? I got a new one last week and I`m...           1.000000   \n\n   predicted_labels  \n0           neutral  \n1          negative  \n2          negative  \n3          negative  \n4          negative  \n5          positive  \n6          positive  \n7          positive  \n8           neutral  \n9          positive  \n10         negative  \n11         positive  \n12         negative  \n13         negative  \n14          neutral  \n15         negative  \n16          neutral  \n17         negative  \n18         positive  \n19         negative  \n20         positive  \n21         positive  \n22         positive  \n23         negative  \n24         negative  \n25         positive  \n26         positive  \n27         negative  \n28         positive  \n29         negative  \n30         positive  \n31         positive  \n32         negative  \n33         positive  \n34          neutral  \n35          neutral  \n36         negative  \n37         positive  \n38         negative  \n39         positive  \n40         negative  \n41         positive  \n42          neutral  \n43         positive  \n44         positive  \n45          neutral  \n46         negative  \n47          neutral  \n48         negative  \n49         negative  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>confidence_scores</th>\n      <th>predicted_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I`d have responded, if I were going</td>\n      <td>1.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>my boss is bullying me...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what interview! leave me alone</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2am feedings for the baby are fun when he is a...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Soooo high</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Both of you</td>\n      <td>1.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>as much as i love to be hopeful, i reckon the...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>I really really like the song Love Story by Ta...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>My Sharpie is running DANGERously low on ink</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>i want to go to music tonight but i lost my vo...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>test test from the LG enV2</td>\n      <td>1.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Uh oh, I am sunburned</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>S`ok, trying to plot alternatives as we speak...</td>\n      <td>0.666667</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>i`ve been sick for the past few days  and thus...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>is back home now      gonna miss every one</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Hes just not that into you</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>oh Marly, I`m so sorry!!  I hope you find her...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Playing Ghost Online is really interesting. Th...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>is cleaning the house for her family who is co...</td>\n      <td>0.666667</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>gotta restart my computer .. I thought Win7 wa...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>the free fillin` app on my ipod is fun, im add...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>I`m sorry.</td>\n      <td>0.333333</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>On the way to Malaysia...no internet access to...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>juss came backk from Berkeleyy ; omg its madd ...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Went to sleep and there is a power cut in Noid...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>I`m going home now. Have you seen my new twitt...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>i hope unni will make the audition . fighting ...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>If it is any consolation I got my BMI tested ...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>That`s very funny.  Cute kids.</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Ahhh, I slept through the game.  I`m gonna tr...</td>\n      <td>0.666667</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Thats it, its the end. Tears for Fears vs Eric...</td>\n      <td>1.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Born and raised in NYC and living in Texas for...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>just in case you wonder, we are really busy to...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>i`m soooooo sleeeeepy!!! the last day o` schoo...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>A little happy for the wine jeje ok it`sm my f...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Car not happy, big big dent in boot! Hoping t...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>im an avid fan of **** magazine and i love you...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>MAYDAY?!</td>\n      <td>1.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>I love to! But I`m only available from 5pm.  ...</td>\n      <td>1.000000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>The girl in the hair salon asked me 'Shall I t...</td>\n      <td>1.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>egh blah and boooooooooooo i dunno wanna go to...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>:visiting my friendster and facebook</td>\n      <td>1.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>i donbt like to peel prawns, i also dont like ...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>which case? I got a new one last week and I`m...</td>\n      <td>1.000000</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T10:52:28.952691100Z",
     "start_time": "2023-08-16T10:52:28.867148500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unannotated = pd.read_csv('data/unannotated/unannotated_sentiment_dataset.csv', encoding= 'unicode_escape', index_col=[0])\n",
    "original_dataset = pd.read_csv('data/original/train.csv', encoding= 'unicode_escape')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('openai/organization.txt', 'r') as file:\n",
    "    openai.organization = file.read().strip()\n",
    "\n",
    "with open('openai/key.txt', 'r') as file:\n",
    "    openai.api_key = file.read().strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accumulated_tokens_method1 = 0\n",
    "accumulated_cost_method1 = 0\n",
    "cost_per_token = 0.0035 / 1000  # The total cost per token, input and output\n",
    "index = 0\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def analyze_ada(text):\n",
    "    global index\n",
    "    global accumulated_tokens_method1\n",
    "    global accumulated_cost_method1\n",
    "\n",
    "    prompt = f\"Sentiment analysis for the following text in a single number: 1 for positive, 0 for neutral, 2 for negative: \\\"{text}\\\"\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-ada-001\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3,\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    total_tokens_used = response['usage']['total_tokens']\n",
    "    print(f\"Total tokens used for this call: {total_tokens_used}\")\n",
    "\n",
    "    call_cost = total_tokens_used * cost_per_token\n",
    "    accumulated_cost_method1 += call_cost\n",
    "    accumulated_tokens_method1 += total_tokens_used\n",
    "    index += 1\n",
    "    print('\\nIndex: ', index)\n",
    "    print(f\"Cost for this call: {call_cost}\")\n",
    "    print(f\"Accumulated tokens so far: {accumulated_tokens_method1}\")\n",
    "    print(f\"Accumulated cost so far: {accumulated_cost_method1}\")\n",
    "\n",
    "    response_text = response.choices[0].text.strip().lower()\n",
    "    print('response: ', response_text)\n",
    "    return response_text\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=120), stop=stop_after_attempt(6))\n",
    "def analyze_gpt35(text):\n",
    "    global index\n",
    "    global accumulated_cost\n",
    "    global accumulated_tokens\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Your task is to analyze text and classify its sentiment as either 'positive', 'negative', or 'neutral' in a single word.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the sentiment of: '{text}'.\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=3,\n",
    "        n=3,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    total_tokens_used = response['usage']['total_tokens']\n",
    "    print(f\"Total tokens used for this call: {total_tokens_used}\")\n",
    "\n",
    "    call_cost = total_tokens_used * cost_per_token\n",
    "    accumulated_cost += call_cost\n",
    "    accumulated_tokens += total_tokens_used\n",
    "    index += 1\n",
    "    print('Index: ', index)\n",
    "    print(f\"Cost for this call: {call_cost}\")\n",
    "    print(f\"Accumulated tokens so far: {accumulated_tokens}\")\n",
    "    print(f\"Accumulated cost so far: {accumulated_cost}\\n\")\n",
    "\n",
    "    response_texts = [choice.message.content.strip().lower() for choice in response.choices]\n",
    "    primary_response = response_texts[0]\n",
    "    confidence_score = response_texts.count(primary_response) / 3\n",
    "\n",
    "    return primary_response, confidence_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data = unannotated.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_rows = 50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data['predicted_gpt35'] = llm_annotated_data['text'].iloc[0:num_rows].apply(analyze_gpt35)\n",
    "#llm_annotated_data['predicted_label'] = llm_annotated_data['text'].iloc[0:num_rows].apply(analyze_ada)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiments_and_scores = llm_annotated_data['text'].iloc[0:num_rows].apply(analyze_gpt35)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data.loc[llm_annotated_data.index[0:num_rows], 'predicted_labels'] = [x[0] for x in sentiments_and_scores]\n",
    "llm_annotated_data.loc[llm_annotated_data.index[0:num_rows], 'confidence_score'] = [x[1] for x in sentiments_and_scores]\n",
    "llm_annotated_data['annotation_correct'] = (llm_annotated_data['predicted_labels'] == original_dataset['sentiment']).astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data.to_csv('data/sentiment/annotated/ada/ada_500.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#original_dataset.loc[0:num_rows - 1, 'sentiment'] = original_dataset['sentiment'].iloc[0:num_rows].map(sentiments)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#original_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data.iloc[0:num_rows]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data['annotation_correct'] = (llm_annotated_data['predicted_gpt35'].iloc[0:num_rows] == original_dataset['sentiment'].iloc[0:num_rows]).astype(int)\n",
    "\n",
    "llm_annotated_data['annotation_correct'] = (llm_annotated_data['predicted_labels'].iloc[0:num_rows].astype(int) == original_dataset['sentiment'].iloc[0:num_rows].astype(int)).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data.iloc[0:num_rows].to_csv('data/sentiment/gpt35_annotated.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data['predicted_gpt35'] = llm_annotated_data['text'].iloc[0:num_rows].apply(analyze_gpt35)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(f\"Accuracy of GPT3.5's annotations: {accuracy_score(original_dataset['sentiment'].iloc[0:num_rows].astype('str').values, llm_annotated_data['predicted_gpt35'].iloc[0:num_rows].astype('str').values)}\")\n",
    "\n",
    "print(f\"Accuracy of Davinci 003's annotations: {accuracy_score(original_dataset['sentiment'].iloc[0:num_rows].astype('str').values, llm_annotated_data['predicted_davinci'].iloc[0:num_rows].astype('str').values)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data.iloc[0:num_rows].to_csv('data/sentiment/gpt35_annotated.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/sentiment/davinci003_annotated_300.csv', index_col=[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset['predicted_davinci'] = dataset['predicted_davinci'].apply(lambda x: x.replace('.', ''))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset['predicted_davinci'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = dataset.copy()\n",
    "sentiments = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "data['predicted_davinci'] = data['predicted_davinci'].map(sentiments)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['predicted_davinci']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler=None):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device, sentiments):\n",
    "    model = model.eval()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return correct_predictions.double() / len(data_loader.dataset), classification_report(real_values, predictions, target_names=sentiments.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_bert(model_path, data_path):\n",
    "\n",
    "    experiment_name = \"llm_seminar_data_annotation\"\n",
    "    client = MlflowClient()\n",
    "    experiment_id = client.get_experiment_by_name(experiment_name)\n",
    "    if experiment_id is None:\n",
    "        experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    else:\n",
    "        experiment_id = experiment_id.experiment_id\n",
    "\n",
    "    model_name = \"_\".join(model_path.split(\"/\")[-1].split(\"_\")[:-2]) # 'bert_sentiment_gpt35_1000' for your example path\n",
    "    with mlflow.start_run(experiment_id=experiment_id):\n",
    "        DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        MODEL_NAME = 'bert-base-uncased'\n",
    "        BATCH_SIZE = 16\n",
    "        EPOCHS = 1\n",
    "\n",
    "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "        mlflow.log_param(\"epochs\", EPOCHS)\n",
    "        mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "\n",
    "        sentiments = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "        data = pd.read_csv(data_path, index_col=[0]) #LLM Annotated Dataset\n",
    "        data['predicted_labels'] = data['predicted_labels'].map(sentiments)\n",
    "\n",
    "        train_texts, val_texts, train_targets, val_targets = train_test_split(data['text'], data['predicted_labels'], test_size=0.2)\n",
    "\n",
    "        train_texts = train_texts.reset_index(drop=True)\n",
    "        val_texts = val_texts.reset_index(drop=True)\n",
    "        train_targets = train_targets.reset_index(drop=True)\n",
    "        val_targets = val_targets.reset_index(drop=True)\n",
    "\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        train_data = SentimentDataset(train_texts, train_targets, tokenizer, max_len=128)\n",
    "        val_data = SentimentDataset(val_texts, val_targets, tokenizer, max_len=128)\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(sentiments)).to(DEVICE)\n",
    "\n",
    "        # Train\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            train_acc, train_loss = train_epoch(model, train_loader, optimizer, DEVICE)\n",
    "            mlflow.log_metric(\"train_acc\", train_acc)\n",
    "            mlflow.log_metric(\"train_loss\", train_loss)\n",
    "\n",
    "            print(f'Train loss: {train_loss}, accuracy: {train_acc}')\n",
    "\n",
    "            val_acc, val_report = eval_model(model, val_loader, DEVICE, sentiments)\n",
    "            mlflow.log_metric(\"val_acc\", val_acc)\n",
    "\n",
    "            print(f'Val accuracy: {val_acc}\\n')\n",
    "            #print(val_report)\n",
    "\n",
    "        torch.save(model, model_path)\n",
    "        result = mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        mlflow.register_model(\n",
    "            model_uri=f\"runs:/{mlflow.active_run().info.run_id}/model\",\n",
    "            name=model_name\n",
    "        )\n",
    "        ALL_RUNS_INFO = client.list_run_infos(experiment_id)\n",
    "        ALL_RUNS_ID = [run.run_id for run in ALL_RUNS_INFO]\n",
    "        ALL_METRIC = [client.get_run(run_id).data.metrics[\"val_acc\"] for run_id in ALL_RUNS_ID]\n",
    "        \n",
    "        runs = pd.DataFrame({\"Run ID\": ALL_RUNS_ID, \"Metrics\": ALL_METRIC})\n",
    "        print(runs.columns)\n",
    "\n",
    "        if runs.empty:\n",
    "            print(\"No runs found.\")\n",
    "            return\n",
    "        \n",
    "        best_run_id = runs.sort_values(\"Metrics\", ascending=False).iloc[0][\"Run ID\"]\n",
    "        best_val_acc = runs.sort_values(\"Metrics\", ascending=False).iloc[0][\"Metrics\"]\n",
    "\n",
    "\n",
    "        # Fetch and load the best model\n",
    "        try:\n",
    "            model_version = client.get_latest_versions(model_name, stages=[\"None\", \"Staging\", \"Production\"])[0].version\n",
    "            model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "            model = mlflow.pytorch.load_model(model_uri)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching model: {e}\")\n",
    "            return\n",
    "\n",
    "        # Print the model details\n",
    "        print(f\"Model Name: {model_name}\")\n",
    "        print(f\"Model Version: {model_version}\")\n",
    "        print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    mlflow.end_run()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_bert('models/bert_sentiment_gpt35_1000_model2.pt', 'data/annotated/gpt35_conf_scores_1000_preproc.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/sentiment/test.csv', encoding= 'unicode_escape')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.drop(columns=[x for x in test.columns if x != 'text' and x != 'sentiment'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.dropna(subset=['sentiment'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.sentiment.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in test.columns:\n",
    "    test[i] = test[i].astype('str')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_texts = test['text'].reset_index(drop=True)\n",
    "test_targets = test['sentiment'].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if isinstance(test_targets[0], str):\n",
    "    label_encoder = LabelEncoder()\n",
    "    test_targets = label_encoder.fit_transform(test_targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = SentimentDataset(test_texts, test_targets, tokenizer, max_len=128)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_acc, test_report = eval_model(model, test_loader, DEVICE)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "print(test_report)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
