{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tenacity import (\n",
    "retry,\n",
    "stop_after_attempt,\n",
    "wait_random_exponential,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unannotated = pd.read_csv('data/unannotated/unannotated_sentiment_dataset.csv', encoding= 'unicode_escape', index_col=[0])\n",
    "original_dataset = pd.read_csv('data/original/train.csv', encoding= 'unicode_escape')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('openai/organization.txt', 'r') as file:\n",
    "    openai.organization = file.read().strip()\n",
    "\n",
    "with open('openai/key.txt', 'r') as file:\n",
    "    openai.api_key = file.read().strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accumulated_tokens_method1 = 0\n",
    "accumulated_cost_method1 = 0\n",
    "cost_per_token = 0.0035 / 1000  # The total cost per token, input and output\n",
    "index = 0\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def analyze_ada(text):\n",
    "    global index\n",
    "    global accumulated_tokens_method1\n",
    "    global accumulated_cost_method1\n",
    "\n",
    "    prompt = f\"Sentiment analysis for the following text in a single number: 1 for positive, 0 for neutral, 2 for negative: \\\"{text}\\\"\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-ada-001\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=3,\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    total_tokens_used = response['usage']['total_tokens']\n",
    "    print(f\"Total tokens used for this call: {total_tokens_used}\")\n",
    "\n",
    "    call_cost = total_tokens_used * cost_per_token\n",
    "    accumulated_cost_method1 += call_cost\n",
    "    accumulated_tokens_method1 += total_tokens_used\n",
    "    index += 1\n",
    "    print('\\nIndex: ', index)\n",
    "    print(f\"Cost for this call: {call_cost}\")\n",
    "    print(f\"Accumulated tokens so far: {accumulated_tokens_method1}\")\n",
    "    print(f\"Accumulated cost so far: {accumulated_cost_method1}\")\n",
    "\n",
    "    response_text = response.choices[0].text.strip().lower()\n",
    "    print('response: ', response_text)\n",
    "    return response_text\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=120), stop=stop_after_attempt(6))\n",
    "def analyze_gpt35(text):\n",
    "    global index\n",
    "    global accumulated_cost\n",
    "    global accumulated_tokens\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Your task is to analyze text and classify its sentiment as either 'positive', 'negative', or 'neutral' in a single word.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the sentiment of: '{text}'.\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=3,\n",
    "        n=3,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    total_tokens_used = response['usage']['total_tokens']\n",
    "    print(f\"Total tokens used for this call: {total_tokens_used}\")\n",
    "\n",
    "    call_cost = total_tokens_used * cost_per_token\n",
    "    accumulated_cost += call_cost\n",
    "    accumulated_tokens += total_tokens_used\n",
    "    index += 1\n",
    "    print('Index: ', index)\n",
    "    print(f\"Cost for this call: {call_cost}\")\n",
    "    print(f\"Accumulated tokens so far: {accumulated_tokens}\")\n",
    "    print(f\"Accumulated cost so far: {accumulated_cost}\\n\")\n",
    "\n",
    "    response_texts = [choice.message.content.strip().lower() for choice in response.choices]\n",
    "    primary_response = response_texts[0]\n",
    "    confidence_score = response_texts.count(primary_response) / 3\n",
    "\n",
    "    return primary_response, confidence_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data = unannotated.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_rows = 50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data['predicted_gpt35'] = llm_annotated_data['text'].iloc[0:num_rows].apply(analyze_gpt35)\n",
    "#llm_annotated_data['predicted_label'] = llm_annotated_data['text'].iloc[0:num_rows].apply(analyze_ada)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiments_and_scores = llm_annotated_data['text'].iloc[0:num_rows].apply(analyze_gpt35)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data.loc[llm_annotated_data.index[0:num_rows], 'predicted_labels'] = [x[0] for x in sentiments_and_scores]\n",
    "llm_annotated_data.loc[llm_annotated_data.index[0:num_rows], 'confidence_score'] = [x[1] for x in sentiments_and_scores]\n",
    "llm_annotated_data['annotation_correct'] = (llm_annotated_data['predicted_labels'] == original_dataset['sentiment']).astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data.to_csv('data/sentiment/annotated/ada/ada_500.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#original_dataset.loc[0:num_rows - 1, 'sentiment'] = original_dataset['sentiment'].iloc[0:num_rows].map(sentiments)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#original_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data.iloc[0:num_rows]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data['annotation_correct'] = (llm_annotated_data['predicted_gpt35'].iloc[0:num_rows] == original_dataset['sentiment'].iloc[0:num_rows]).astype(int)\n",
    "\n",
    "llm_annotated_data['annotation_correct'] = (llm_annotated_data['predicted_labels'].iloc[0:num_rows].astype(int) == original_dataset['sentiment'].iloc[0:num_rows].astype(int)).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_annotated_data.iloc[0:num_rows].to_csv('data/sentiment/gpt35_annotated.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data['predicted_gpt35'] = llm_annotated_data['text'].iloc[0:num_rows].apply(analyze_gpt35)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(f\"Accuracy of GPT3.5's annotations: {accuracy_score(original_dataset['sentiment'].iloc[0:num_rows].astype('str').values, llm_annotated_data['predicted_gpt35'].iloc[0:num_rows].astype('str').values)}\")\n",
    "\n",
    "print(f\"Accuracy of Davinci 003's annotations: {accuracy_score(original_dataset['sentiment'].iloc[0:num_rows].astype('str').values, llm_annotated_data['predicted_davinci'].iloc[0:num_rows].astype('str').values)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#llm_annotated_data.iloc[0:num_rows].to_csv('data/sentiment/gpt35_annotated.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/sentiment/davinci003_annotated_300.csv', index_col=[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset['predicted_davinci'] = dataset['predicted_davinci'].apply(lambda x: x.replace('.', ''))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset['predicted_davinci'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = dataset.copy()\n",
    "sentiments = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "data['predicted_davinci'] = data['predicted_davinci'].map(sentiments)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['predicted_davinci']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler=None):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device, sentiments):\n",
    "    model = model.eval()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return correct_predictions.double() / len(data_loader.dataset), classification_report(real_values, predictions, target_names=sentiments.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_bert(model_path, data_path):\n",
    "\n",
    "    experiment_name = \"llm_seminar_data_annotation\"\n",
    "    client = MlflowClient()\n",
    "    experiment_id = client.get_experiment_by_name(experiment_name)\n",
    "    if experiment_id is None:\n",
    "        experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    else:\n",
    "        experiment_id = experiment_id.experiment_id\n",
    "\n",
    "    model_name = \"_\".join(model_path.split(\"/\")[-1].split(\"_\")[:-2]) # 'bert_sentiment_gpt35_1000' for your example path\n",
    "    with mlflow.start_run(experiment_id=experiment_id):\n",
    "        DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        MODEL_NAME = 'bert-base-uncased'\n",
    "        BATCH_SIZE = 16\n",
    "        EPOCHS = 1\n",
    "\n",
    "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "        mlflow.log_param(\"epochs\", EPOCHS)\n",
    "        mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "\n",
    "        sentiments = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "        data = pd.read_csv(data_path, index_col=[0]) #LLM Annotated Dataset\n",
    "        data['predicted_labels'] = data['predicted_labels'].map(sentiments)\n",
    "\n",
    "        train_texts, val_texts, train_targets, val_targets = train_test_split(data['text'], data['predicted_labels'], test_size=0.2)\n",
    "\n",
    "        train_texts = train_texts.reset_index(drop=True)\n",
    "        val_texts = val_texts.reset_index(drop=True)\n",
    "        train_targets = train_targets.reset_index(drop=True)\n",
    "        val_targets = val_targets.reset_index(drop=True)\n",
    "\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        train_data = SentimentDataset(train_texts, train_targets, tokenizer, max_len=128)\n",
    "        val_data = SentimentDataset(val_texts, val_targets, tokenizer, max_len=128)\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(sentiments)).to(DEVICE)\n",
    "\n",
    "        # Train\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            train_acc, train_loss = train_epoch(model, train_loader, optimizer, DEVICE)\n",
    "            mlflow.log_metric(\"train_acc\", train_acc)\n",
    "            mlflow.log_metric(\"train_loss\", train_loss)\n",
    "\n",
    "            print(f'Train loss: {train_loss}, accuracy: {train_acc}')\n",
    "\n",
    "            val_acc, val_report = eval_model(model, val_loader, DEVICE, sentiments)\n",
    "            mlflow.log_metric(\"val_acc\", val_acc)\n",
    "\n",
    "            print(f'Val accuracy: {val_acc}\\n')\n",
    "            #print(val_report)\n",
    "\n",
    "        torch.save(model, model_path)\n",
    "        result = mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        mlflow.register_model(\n",
    "            model_uri=f\"runs:/{mlflow.active_run().info.run_id}/model\",\n",
    "            name=model_name\n",
    "        )\n",
    "        ALL_RUNS_INFO = client.list_run_infos(experiment_id)\n",
    "        ALL_RUNS_ID = [run.run_id for run in ALL_RUNS_INFO]\n",
    "        ALL_METRIC = [client.get_run(run_id).data.metrics[\"val_acc\"] for run_id in ALL_RUNS_ID]\n",
    "        \n",
    "        runs = pd.DataFrame({\"Run ID\": ALL_RUNS_ID, \"Metrics\": ALL_METRIC})\n",
    "        print(runs.columns)\n",
    "\n",
    "        if runs.empty:\n",
    "            print(\"No runs found.\")\n",
    "            return\n",
    "        \n",
    "        best_run_id = runs.sort_values(\"Metrics\", ascending=False).iloc[0][\"Run ID\"]\n",
    "        best_val_acc = runs.sort_values(\"Metrics\", ascending=False).iloc[0][\"Metrics\"]\n",
    "\n",
    "\n",
    "        # Fetch and load the best model\n",
    "        try:\n",
    "            model_version = client.get_latest_versions(model_name, stages=[\"None\", \"Staging\", \"Production\"])[0].version\n",
    "            model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "            model = mlflow.pytorch.load_model(model_uri)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching model: {e}\")\n",
    "            return\n",
    "\n",
    "        # Print the model details\n",
    "        print(f\"Model Name: {model_name}\")\n",
    "        print(f\"Model Version: {model_version}\")\n",
    "        print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    mlflow.end_run()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_bert('models/bert_sentiment_gpt35_1000_model2.pt', 'data/annotated/gpt35_conf_scores_1000_preproc.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/sentiment/test.csv', encoding= 'unicode_escape')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.drop(columns=[x for x in test.columns if x != 'text' and x != 'sentiment'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.dropna(subset=['sentiment'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.sentiment.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in test.columns:\n",
    "    test[i] = test[i].astype('str')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_texts = test['text'].reset_index(drop=True)\n",
    "test_targets = test['sentiment'].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if isinstance(test_targets[0], str):\n",
    "    label_encoder = LabelEncoder()\n",
    "    test_targets = label_encoder.fit_transform(test_targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = SentimentDataset(test_texts, test_targets, tokenizer, max_len=128)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_acc, test_report = eval_model(model, test_loader, DEVICE)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "print(test_report)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
