{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:00:54.403760600Z",
     "start_time": "2023-08-28T05:00:46.431646100Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.base import BaseEstimator\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from cleanlab.classification import CleanLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler=None):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:00:54.449882100Z",
     "start_time": "2023-08-28T05:00:54.405771600Z"
    }
   },
   "id": "d7f8ca33ebed311d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class BertSentimentClassifier(BaseEstimator):\n",
    "    def __init__(self, model_path='bert-base-uncased', device=None):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.max_len=128\n",
    "\n",
    "    def fit(self, X, y, epochs=3):\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        train_data = SentimentDataset(X, y, self.tokenizer, max_len=self.max_len)\n",
    "        train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "        \n",
    "        optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_acc, train_loss = train_epoch(self.model, train_loader, optimizer, self.device)\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Train loss: {train_loss}, accuracy: {train_acc}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_list = X.tolist()\n",
    "        encoding = self.tokenizer.batch_encode_plus(\n",
    "            X_list,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "        \n",
    "        return self.classes_[preds.cpu().numpy()]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_list = X.tolist()  # Convert to list\n",
    "        encoding = self.tokenizer.batch_encode_plus(\n",
    "            X_list,  # Updated this line\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        \n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = (y_pred == y).mean()\n",
    "        return accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:00:57.477556900Z",
     "start_time": "2023-08-28T05:00:57.446447600Z"
    }
   },
   "id": "f2843a668f597507"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/merged/merged_6.csv', encoding='unicode_escape')[0:1500]\n",
    "\n",
    "#data.drop(columns=['textID', 'selected_text', 'Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)'], inplace=True)\n",
    "\n",
    "data = data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:52:41.553435900Z",
     "start_time": "2023-08-28T05:52:41.521913300Z"
    }
   },
   "id": "109ce8b792b915ca"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  text  confidence_scores  \\\n0                  I`d have responded, if I were going                1.0   \n1        Sooo SAD I will miss you here in San Diego!!!                1.0   \n2                            my boss is bullying me...                1.0   \n3                       what interview! leave me alone                1.0   \n4     Sons of ****, why couldn`t they put them on t...                1.0   \n..                                                 ...                ...   \n194                                      i talk to you                1.0   \n195  im soo bored...im deffo missing my music channels                1.0   \n196           nite nite bday girl  have fun at concert                1.0   \n197  Had nicotine replacement patch on for 4 hours....                1.0   \n198  _Sanderson What`s with Twatter lately?  Either...                1.0   \n\n    predicted_labels  \n0            neutral  \n1           negative  \n2           negative  \n3           negative  \n4           negative  \n..               ...  \n194          neutral  \n195         negative  \n196         positive  \n197         negative  \n198         negative  \n\n[199 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>confidence_scores</th>\n      <th>predicted_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I`d have responded, if I were going</td>\n      <td>1.0</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>1.0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>my boss is bullying me...</td>\n      <td>1.0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what interview! leave me alone</td>\n      <td>1.0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>1.0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>i talk to you</td>\n      <td>1.0</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>im soo bored...im deffo missing my music channels</td>\n      <td>1.0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>nite nite bday girl  have fun at concert</td>\n      <td>1.0</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>Had nicotine replacement patch on for 4 hours....</td>\n      <td>1.0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>_Sanderson What`s with Twatter lately?  Either...</td>\n      <td>1.0</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>199 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:52:41.833860700Z",
     "start_time": "2023-08-28T05:52:41.787610500Z"
    }
   },
   "id": "5f9507ae14d2af30"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m clf \u001B[38;5;241m=\u001B[39m \u001B[43mBertSentimentClassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \n",
      "Cell \u001B[1;32mIn[5], line 7\u001B[0m, in \u001B[0;36mBertSentimentClassifier.__init__\u001B[1;34m(self, model_path, device)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m BertForSequenceClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_path, num_labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m device \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m\n",
      "File \u001B[1;32m~\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:1900\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1895\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1896\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1897\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1898\u001B[0m     )\n\u001B[0;32m   1899\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1900\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32m~\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "clf = BertSentimentClassifier() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:52:45.421925900Z",
     "start_time": "2023-08-28T05:52:42.054794800Z"
    }
   },
   "id": "57c1bccde4f474ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_texts, raw_labels = data[\"text\"].values, data[\"predicted_labels\"].values\n",
    "raw_train_texts, raw_test_texts, raw_train_labels, raw_test_labels = train_test_split(raw_texts, raw_labels, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:52:45.423926800Z",
     "start_time": "2023-08-28T05:52:45.421925900Z"
    }
   },
   "id": "aed194597e45af54"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "predicted_labels\nnegative    79\npositive    76\nneutral     44\nName: count, dtype: int64"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.predicted_labels.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:52:46.035384700Z",
     "start_time": "2023-08-28T05:52:45.999801700Z"
    }
   },
   "id": "bcc122de56c87d80"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 4 classes.\n",
      "Classes: {'positive', 'mixed', 'neutral', 'negative'}\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(raw_train_labels))\n",
    "\n",
    "print(f\"This dataset has {num_classes} classes.\")\n",
    "print(f\"Classes: {set(raw_train_labels)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:46:36.674731800Z",
     "start_time": "2023-08-28T05:46:36.646629600Z"
    }
   },
   "id": "dfc7a14d56e940d5"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#Label encoding the labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(raw_train_labels)\n",
    "\n",
    "train_labels = encoder.transform(raw_train_labels)\n",
    "test_labels = encoder.transform(raw_test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:52:22.797216900Z",
     "start_time": "2023-08-28T05:52:22.770952900Z"
    }
   },
   "id": "ab5e9d39280dfd9d"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saran\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/10 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_train_texts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# Initial model prediction , without removing noisy labels\u001B[39;00m\n\u001B[0;32m      2\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mscore(raw_test_texts, test_labels) \u001B[38;5;66;03m# This gives the baseline accuracy without acting on noisy labels\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[5], line 19\u001B[0m, in \u001B[0;36mBertSentimentClassifier.fit\u001B[1;34m(self, X, y, epochs)\u001B[0m\n\u001B[0;32m     16\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m AdamW(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m---> 19\u001B[0m     train_acc, train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Train loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[4], line 49\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, data_loader, optimizer, device, scheduler)\u001B[0m\n\u001B[0;32m     46\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     47\u001B[0m correct_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(preds \u001B[38;5;241m==\u001B[39m targets)\n\u001B[1;32m---> 49\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     51\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     52\u001B[0m nn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "clf.fit(raw_train_texts, train_labels, 3) # Initial model prediction , without removing noisy labels\n",
    "accuracy = clf.score(raw_test_texts, test_labels) # This gives the baseline accuracy without acting on noisy labels\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:52:32.153805600Z",
     "start_time": "2023-08-28T05:52:25.437243100Z"
    }
   },
   "id": "476c0e3863a52d79"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "cv_n_folds = 3  # values like 5 or 10 will generally work better\n",
    "cl = CleanLearning(clf, cv_n_folds=cv_n_folds) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T03:17:23.783979500Z",
     "start_time": "2023-08-28T03:17:23.757662900Z"
    }
   },
   "id": "8c93cc0ba4da5518"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\saran\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 50/50 [00:13<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train loss: 1.0438781011104583, accuracy: 0.4355444305381727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train loss: 0.7833910012245178, accuracy: 0.6795994993742178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train loss: 0.5369862079620361, accuracy: 0.8085106382978724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 50/50 [00:14<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train loss: 1.057125790119171, accuracy: 0.41551939924906134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:18<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train loss: 0.8346887052059173, accuracy: 0.623279098873592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:19<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train loss: 0.5958329010009765, accuracy: 0.7684605757196495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 50/50 [00:17<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train loss: 1.0601495099067688, accuracy: 0.45125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train loss: 0.7712335515022278, accuracy: 0.69125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:19<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train loss: 0.48221698552370074, accuracy: 0.8225\n"
     ]
    }
   ],
   "source": [
    "label_issues = cl.find_label_issues(X=raw_train_texts, labels=train_labels) # Finding label issues in dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T03:21:58.910821700Z",
     "start_time": "2023-08-28T03:17:28.233328Z"
    }
   },
   "id": "5fe1c7ae4880c4c6"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "identified_issues = label_issues[label_issues[\"is_label_issue\"] == True] \n",
    "lowest_quality_labels = label_issues[\"label_quality\"].argsort().to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T03:21:58.927706300Z",
     "start_time": "2023-08-28T03:21:58.912824Z"
    }
   },
   "id": "71d89b8a869b5d29"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def print_as_df(index):\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"text\": raw_train_texts,\n",
    "            \"given_label\": raw_train_labels,\n",
    "            \"predicted_label\": encoder.inverse_transform(label_issues[\"predicted_label\"]),\n",
    "            \"quality\": label_issues[\"label_quality\"]\n",
    "        },\n",
    "    ).iloc[index]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T03:21:58.955211400Z",
     "start_time": "2023-08-28T03:21:58.928706200Z"
    }
   },
   "id": "f225fe751eaff9fb"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text given_label  \\\n178    but my bday is JUNE 19.. this is wack... and ...    negative   \n190   okie gonna tweet more because i am loosing you...    negative   \n95     I am definitely ready... actually ahead of yo...    negative   \n250     Thanks! My mom`s seed is larger and already ...    negative   \n433                  ahaha its stuck in my head; thanxx    positive   \n1139  Really wishes he had some spare cash to buy th...    negative   \n230    lmao yeaa iight n u shuld put tha c.b flick u...    positive   \n640       Flap-a-taco was nice until the plebs came in.    negative   \n368    did you see the 15 sec clip of the New Moon t...    negative   \n1113                                  so very irratated    negative   \n14    A sunny morning in the Big K, with lawns to mo...    negative   \n445   almost died. Laptop screen was set to 100% bri...    positive   \n566             And the Sun is shinning.........at last    positive   \n611   _3 U know - kids do what we DO - not what we S...    positive   \n99    gettn ready to take a trip to Jersey my dad`s ...    negative   \n354                      im sure jont wont mind sharing    positive   \n65     Miley, I tried voting for you, and it wont le...    negative   \n184   msn-ing. no school  shouldn`t there be more ba...    positive   \n760   always excited for a new chapter in my life. j...    negative   \n497   is tired but happy the orphanage was wow.. and...     neutral   \n936              melissa_leah: my car wont start.......    negative   \n796    Yes...met Jon last yr at an interesting 'Ques...    positive   \n1176               Ok that **** duet was hysterical LOL    positive   \n1148   Congrats!  I cuss like that in a matter of mi...     neutral   \n50                                       Not a prob hun    positive   \n882   RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED...     neutral   \n302    While im stuck INSIDE in Elk Grove Village wo...     neutral   \n509    hahahaha wtf dianne????? who twitters that lo...    negative   \n120   if someone had`ve told me things would get thi...    positive   \n257    at first i thought bar life meant you were pa...    positive   \n754    She`s unassuming and unpretentious. She`s jus...    positive   \n1124  I just spent 2 hours looking for a blog topic ...    negative   \n939    someone came in when I was sleeping off my na...    positive   \n369   My modem has been offline for a week now... Go...    negative   \n1111     Epicfail and Joseph in class... Very long liao    negative   \n1042  i hope it doesnt rain tonight tomorrow my fam....    positive   \n100    boo... i was hoping for a fake alien story wi...    negative   \n654    WOW never seen him before , he`s bloody aweso...     neutral   \n1153  _Divine take my advice lookin through the phon...    positive   \n504                      noone wants to talk to me  lol    negative   \n692             DUSTBIN BABY ON AT 11.30 Cannot wait  x    positive   \n164   You better come here by the time I count to 10...    positive   \n781   I`m likable after all HAHAHAHAHAHAHA! Still ov...    positive   \n516        We are of like minds this evening , my dear!     neutral   \n\n     predicted_label   quality  \n178         positive  0.013122  \n190         positive  0.014401  \n95          positive  0.015469  \n250         positive  0.018361  \n433         negative  0.018489  \n1139        positive  0.018788  \n230          neutral  0.019159  \n640         positive  0.021892  \n368          neutral  0.022637  \n1113        positive  0.022738  \n14          positive  0.022805  \n445         negative  0.023247  \n566          neutral  0.025315  \n611          neutral  0.025812  \n99           neutral  0.027071  \n354          neutral  0.029474  \n65           neutral  0.029613  \n184          neutral  0.029829  \n760         positive  0.031064  \n497         positive  0.032674  \n936          neutral  0.032823  \n796          neutral  0.033030  \n1176        negative  0.033377  \n1148        positive  0.034024  \n50           neutral  0.035646  \n882         positive  0.035845  \n302         positive  0.037135  \n509          neutral  0.037584  \n120         negative  0.039304  \n257          neutral  0.039503  \n754         negative  0.040340  \n1124         neutral  0.040753  \n939          neutral  0.041462  \n369         positive  0.042404  \n1111        positive  0.042514  \n1042        negative  0.043344  \n100          neutral  0.045834  \n654         positive  0.046137  \n1153         neutral  0.048596  \n504          neutral  0.049894  \n692          neutral  0.051455  \n164          neutral  0.051871  \n781          neutral  0.052240  \n516         positive  0.053011  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>given_label</th>\n      <th>predicted_label</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>178</th>\n      <td>but my bday is JUNE 19.. this is wack... and ...</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.013122</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>okie gonna tweet more because i am loosing you...</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.014401</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>I am definitely ready... actually ahead of yo...</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.015469</td>\n    </tr>\n    <tr>\n      <th>250</th>\n      <td>Thanks! My mom`s seed is larger and already ...</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.018361</td>\n    </tr>\n    <tr>\n      <th>433</th>\n      <td>ahaha its stuck in my head; thanxx</td>\n      <td>positive</td>\n      <td>negative</td>\n      <td>0.018489</td>\n    </tr>\n    <tr>\n      <th>1139</th>\n      <td>Really wishes he had some spare cash to buy th...</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.018788</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>lmao yeaa iight n u shuld put tha c.b flick u...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.019159</td>\n    </tr>\n    <tr>\n      <th>640</th>\n      <td>Flap-a-taco was nice until the plebs came in.</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.021892</td>\n    </tr>\n    <tr>\n      <th>368</th>\n      <td>did you see the 15 sec clip of the New Moon t...</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.022637</td>\n    </tr>\n    <tr>\n      <th>1113</th>\n      <td>so very irratated</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.022738</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>A sunny morning in the Big K, with lawns to mo...</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.022805</td>\n    </tr>\n    <tr>\n      <th>445</th>\n      <td>almost died. Laptop screen was set to 100% bri...</td>\n      <td>positive</td>\n      <td>negative</td>\n      <td>0.023247</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>And the Sun is shinning.........at last</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.025315</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>_3 U know - kids do what we DO - not what we S...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.025812</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>gettn ready to take a trip to Jersey my dad`s ...</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.027071</td>\n    </tr>\n    <tr>\n      <th>354</th>\n      <td>im sure jont wont mind sharing</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.029474</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>Miley, I tried voting for you, and it wont le...</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.029613</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>msn-ing. no school  shouldn`t there be more ba...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.029829</td>\n    </tr>\n    <tr>\n      <th>760</th>\n      <td>always excited for a new chapter in my life. j...</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.031064</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>is tired but happy the orphanage was wow.. and...</td>\n      <td>neutral</td>\n      <td>positive</td>\n      <td>0.032674</td>\n    </tr>\n    <tr>\n      <th>936</th>\n      <td>melissa_leah: my car wont start.......</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.032823</td>\n    </tr>\n    <tr>\n      <th>796</th>\n      <td>Yes...met Jon last yr at an interesting 'Ques...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.033030</td>\n    </tr>\n    <tr>\n      <th>1176</th>\n      <td>Ok that **** duet was hysterical LOL</td>\n      <td>positive</td>\n      <td>negative</td>\n      <td>0.033377</td>\n    </tr>\n    <tr>\n      <th>1148</th>\n      <td>Congrats!  I cuss like that in a matter of mi...</td>\n      <td>neutral</td>\n      <td>positive</td>\n      <td>0.034024</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Not a prob hun</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.035646</td>\n    </tr>\n    <tr>\n      <th>882</th>\n      <td>RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED...</td>\n      <td>neutral</td>\n      <td>positive</td>\n      <td>0.035845</td>\n    </tr>\n    <tr>\n      <th>302</th>\n      <td>While im stuck INSIDE in Elk Grove Village wo...</td>\n      <td>neutral</td>\n      <td>positive</td>\n      <td>0.037135</td>\n    </tr>\n    <tr>\n      <th>509</th>\n      <td>hahahaha wtf dianne????? who twitters that lo...</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.037584</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>if someone had`ve told me things would get thi...</td>\n      <td>positive</td>\n      <td>negative</td>\n      <td>0.039304</td>\n    </tr>\n    <tr>\n      <th>257</th>\n      <td>at first i thought bar life meant you were pa...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.039503</td>\n    </tr>\n    <tr>\n      <th>754</th>\n      <td>She`s unassuming and unpretentious. She`s jus...</td>\n      <td>positive</td>\n      <td>negative</td>\n      <td>0.040340</td>\n    </tr>\n    <tr>\n      <th>1124</th>\n      <td>I just spent 2 hours looking for a blog topic ...</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.040753</td>\n    </tr>\n    <tr>\n      <th>939</th>\n      <td>someone came in when I was sleeping off my na...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.041462</td>\n    </tr>\n    <tr>\n      <th>369</th>\n      <td>My modem has been offline for a week now... Go...</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.042404</td>\n    </tr>\n    <tr>\n      <th>1111</th>\n      <td>Epicfail and Joseph in class... Very long liao</td>\n      <td>negative</td>\n      <td>positive</td>\n      <td>0.042514</td>\n    </tr>\n    <tr>\n      <th>1042</th>\n      <td>i hope it doesnt rain tonight tomorrow my fam....</td>\n      <td>positive</td>\n      <td>negative</td>\n      <td>0.043344</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>boo... i was hoping for a fake alien story wi...</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.045834</td>\n    </tr>\n    <tr>\n      <th>654</th>\n      <td>WOW never seen him before , he`s bloody aweso...</td>\n      <td>neutral</td>\n      <td>positive</td>\n      <td>0.046137</td>\n    </tr>\n    <tr>\n      <th>1153</th>\n      <td>_Divine take my advice lookin through the phon...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.048596</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>noone wants to talk to me  lol</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.049894</td>\n    </tr>\n    <tr>\n      <th>692</th>\n      <td>DUSTBIN BABY ON AT 11.30 Cannot wait  x</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.051455</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>You better come here by the time I count to 10...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.051871</td>\n    </tr>\n    <tr>\n      <th>781</th>\n      <td>I`m likable after all HAHAHAHAHAHAHA! Still ov...</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.052240</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>We are of like minds this evening , my dear!</td>\n      <td>neutral</td>\n      <td>positive</td>\n      <td>0.053011</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(lowest_quality_labels[:10]) # Prints 10 labels with the least quality "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T03:21:58.973283900Z",
     "start_time": "2023-08-28T03:21:58.944684800Z"
    }
   },
   "id": "f8709d87b14b6af1"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saran\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 56/56 [00:22<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train loss: 0.031053377356978933, accuracy: 0.988826815642458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:21<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train loss: 0.03172345966491515, accuracy: 0.994413407821229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:25<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train loss: 0.024068544668677663, accuracy: 0.9955307262569831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated_clf = cl.fit(X=raw_train_texts, labels=train_labels, label_issues=cl.get_label_issues()) #New model fitting by pruning error labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T03:23:08.663929300Z",
     "start_time": "2023-08-28T03:21:58.974795700Z"
    }
   },
   "id": "21c47bbb0acb9aff"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "accuracy = updated_clf.score(raw_test_texts, test_labels)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T03:23:26.226938800Z",
     "start_time": "2023-08-28T03:23:08.670832400Z"
    }
   },
   "id": "27c01b3ab497ff39"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'def iterative_cleanlab_training(raw_train_texts, train_labels, raw_test_texts, test_labels, max_iterations=3, tolerance=10):\\n    clf = BertSentimentClassifier()\\n    prev_accuracy = 0\\n    for iteration in range(max_iterations):\\n        print(f\"Starting Iteration {iteration + 1}\")\\n        \\n        # 1. Train the model on the current labeled data\\n        clf.fit(raw_train_texts, train_labels, epochs=3)\\n        accuracy = clf.score(raw_test_texts, test_labels)\\n        print(f\\'Accuracy in Iteration {iteration + 1}: {accuracy:.4f}\\')\\n        \\n        # Stopping condition based on performance change\\n        #if abs(accuracy - prev_accuracy) < tolerance:\\n        #    break\\n        prev_accuracy = accuracy\\n\\n        # 2. Use CleanLab to detect noisy labels\\n        cl = CleanLearning(clf, cv_n_folds=cv_n_folds)\\n        label_issues = cl.find_label_issues(X=raw_train_texts, labels=train_labels)\\n\\n        # 3. Remove or correct the identified noisy labels (here, I\\'m just removing them)\\n        noisy_indices = label_issues[label_issues[\"is_label_issue\"] == True].index\\n        raw_train_texts = np.delete(raw_train_texts, noisy_indices)\\n        train_labels = np.delete(train_labels, noisy_indices)\\n        \\n    return clf'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def iterative_cleanlab_training(raw_train_texts, train_labels, raw_test_texts, test_labels, max_iterations=3, tolerance=10):\n",
    "    clf = BertSentimentClassifier()\n",
    "    prev_accuracy = 0\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Starting Iteration {iteration + 1}\")\n",
    "        \n",
    "        # 1. Train the model on the current labeled data\n",
    "        clf.fit(raw_train_texts, train_labels, epochs=3)\n",
    "        accuracy = clf.score(raw_test_texts, test_labels)\n",
    "        print(f'Accuracy in Iteration {iteration + 1}: {accuracy:.4f}')\n",
    "        \n",
    "        # Stopping condition based on performance change\n",
    "        #if abs(accuracy - prev_accuracy) < tolerance:\n",
    "        #    break\n",
    "        prev_accuracy = accuracy\n",
    "\n",
    "        # 2. Use CleanLab to detect noisy labels\n",
    "        cl = CleanLearning(clf, cv_n_folds=cv_n_folds)\n",
    "        label_issues = cl.find_label_issues(X=raw_train_texts, labels=train_labels)\n",
    "\n",
    "        # 3. Remove or correct the identified noisy labels (here, I'm just removing them)\n",
    "        noisy_indices = label_issues[label_issues[\"is_label_issue\"] == True].index\n",
    "        raw_train_texts = np.delete(raw_train_texts, noisy_indices)\n",
    "        train_labels = np.delete(train_labels, noisy_indices)\n",
    "        \n",
    "    return clf'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:00:42.418685600Z",
     "start_time": "2023-08-28T05:00:42.348361400Z"
    }
   },
   "id": "3150dee8608e9d8a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "\"# Calling the function\\nclf_final = iterative_cleanlab_training(raw_train_texts, train_labels, raw_test_texts, test_labels)\\nfinal_accuracy = clf_final.score(raw_test_texts, test_labels)\\nprint(f'Final Accuracy: {final_accuracy:.4f}')\\n\""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Calling the function\n",
    "clf_final = iterative_cleanlab_training(raw_train_texts, train_labels, raw_test_texts, test_labels)\n",
    "final_accuracy = clf_final.score(raw_test_texts, test_labels)\n",
    "print(f'Final Accuracy: {final_accuracy:.4f}')\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T05:00:42.419686500Z",
     "start_time": "2023-08-28T05:00:42.374605100Z"
    }
   },
   "id": "41d59c7e4bef50ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "accab24bd7c2112e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
