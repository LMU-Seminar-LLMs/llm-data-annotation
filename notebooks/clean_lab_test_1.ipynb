{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from cleanlab import Datalab\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T22:52:12.475710800Z",
     "start_time": "2023-08-27T22:52:12.459708300Z"
    }
   },
   "id": "fcdcc1c32b355db0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T23:29:01.472857800Z",
     "start_time": "2023-08-27T23:28:53.722402100Z"
    }
   },
   "id": "ffe3636fde9a30e4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T23:29:01.558051300Z",
     "start_time": "2023-08-27T23:29:01.470562Z"
    }
   },
   "id": "d083822715cf99d9"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/original/train.csv', encoding = 'unicode_escape')[0:1500]\n",
    "#sentiments = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "#data['sentiment'] = data['sentiment'].map(sentiments)\n",
    "data.drop( columns=['textID', 'selected_text', 'Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)'], inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T23:37:18.222589700Z",
     "start_time": "2023-08-27T23:37:18.075751700Z"
    }
   },
   "id": "5497b0380cef8c06"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text sentiment\n0                   I`d have responded, if I were going   neutral\n1         Sooo SAD I will miss you here in San Diego!!!  negative\n2                             my boss is bullying me...  negative\n3                        what interview! leave me alone  negative\n4      Sons of ****, why couldn`t they put them on t...  negative\n...                                                 ...       ...\n1495      watching crimson tide and eating apple sauce.   neutral\n1496                vocÃ¯Â¿Â½ que sumiu forever do msn.   neutral\n1497  guess I`ll start getting some of my camping an...   neutral\n1498               I miss rollerblading down Shoreline.  negative\n1499  WORS bike race at camp this weekend=a total of...   neutral\n\n[1500 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>my boss is bullying me...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what interview! leave me alone</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>watching crimson tide and eating apple sauce.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>vocÃ¯Â¿Â½ que sumiu forever do msn.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>guess I`ll start getting some of my camping an...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>I miss rollerblading down Shoreline.</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>WORS bike race at camp this weekend=a total of...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T23:37:18.449063700Z",
     "start_time": "2023-08-27T23:37:18.422487700Z"
    }
   },
   "id": "f99bd0ac57b8d0c5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler=None):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device, sentiments):\n",
    "    model = model.eval()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return correct_predictions.double() / len(data_loader.dataset), classification_report(real_values, predictions, target_names=sentiments.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T23:32:27.673500300Z",
     "start_time": "2023-08-27T23:32:26.712096300Z"
    }
   },
   "id": "5814e34ec90819d1"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def train_bert(data, epoch_input):\n",
    "\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    MODEL_NAME = 'bert-base-uncased'\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = epoch_input\n",
    "\n",
    "    sentiments = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "\n",
    "    data = data.copy() #LLM Annotated Dataset\n",
    "    data['sentiment'] = data['sentiment'].map(sentiments)\n",
    "\n",
    "    train_texts, val_texts, train_targets, val_targets = train_test_split(data['text'], data['sentiment'], test_size=0.2)\n",
    "\n",
    "    train_texts = train_texts.reset_index(drop=True)\n",
    "    val_texts = val_texts.reset_index(drop=True)\n",
    "    train_targets = train_targets.reset_index(drop=True)\n",
    "    val_targets = val_targets.reset_index(drop=True)\n",
    "\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    train_data = SentimentDataset(train_texts, train_targets, tokenizer, max_len=128)\n",
    "    val_data = SentimentDataset(val_texts, val_targets, tokenizer, max_len=128)\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(sentiments)).to(DEVICE)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "        print('-' * 10)\n",
    "        train_acc, train_loss = train_epoch(model, train_loader, optimizer, DEVICE)\n",
    "        print(f'Train loss: {train_loss}, accuracy: {train_acc}')\n",
    "        val_acc, val_report = eval_model(model, val_loader, DEVICE, sentiments)\n",
    "        print(f'Val accuracy: {val_acc}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T23:38:12.427596300Z",
     "start_time": "2023-08-27T23:38:12.393784700Z"
    }
   },
   "id": "b71a2e972420d235"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\saran\\Desktop\\LLM Seminar\\Apps Phase\\LLM_Data_Annotation\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:25<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0695697085062663, accuracy: 0.39583333333333337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.5833333333333334\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:23<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7728401648998261, accuracy: 0.6825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.7466666666666667\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:23<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4862236334880193, accuracy: 0.8241666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.7766666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_bert(data, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T23:39:48.532406800Z",
     "start_time": "2023-08-27T23:38:27.577786800Z"
    }
   },
   "id": "4b16b451100155a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "367c0153471adc74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
